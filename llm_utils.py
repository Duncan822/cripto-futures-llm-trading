import requests
from typing import List, Dict

def query_ollama(prompt: str, model: str = "mistral", timeout: int = 1800) -> str:
    """
    Invia un prompt all'istanza Ollama locale e restituisce la risposta.
    Ottimizzato per velocit√† e trading futures.
    
    Args:
        prompt: Il prompt da inviare al modello
        model: Il nome del modello da utilizzare
        timeout: Timeout in secondi (default: 1800 = 30 minuti)
    """
    url = "http://localhost:11434/api/generate"
    
    # Configurazioni ottimizzate per velocit√†
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.3,        # Pi√π basso = pi√π deterministico e veloce
            "top_p": 0.8,             # Pi√π basso = pi√π focalizzato
            "top_k": 40,              # Limita le scelte
            "num_predict": 1024,      # Ridotto per velocit√†
            "repeat_penalty": 1.1,    # Evita ripetizioni
            "num_ctx": 2048,          # Contesto ridotto per velocit√†
            "num_thread": 8,          # Usa pi√π thread se disponibili
            "num_gpu": 1,             # Usa GPU se disponibile
            "num_batch": 512,         # Batch size ottimizzato
            "rope_freq_base": 10000,  # Parametri ROPE ottimizzati
            "rope_freq_scale": 0.5
        }
    }
    
    try:
        print(f"ü§ñ Invio richiesta a {model} (configurazione veloce)...")
        response = requests.post(url, json=payload, timeout=timeout)
        response.raise_for_status()
        result = response.json()
        print(f"‚úÖ Risposta ricevuta da {model}")
        return result["response"]
    except requests.exceptions.Timeout:
        print(f"‚è∞ Timeout per {model} dopo {timeout} secondi")
        raise
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Errore nella richiesta a {model}: {e}")
        raise

def query_ollama_unlimited(prompt: str, model: str = "mistral") -> str:
    """
    Versione senza timeout per cooperazione libera tra LLM.
    Permette ai modelli di prendersi tutto il tempo necessario per generare strategie complesse.
    
    Args:
        prompt: Il prompt da inviare al modello
        model: Il nome del modello da utilizzare
        
    Returns:
        Risposta del modello senza limiti di tempo
    """
    url = "http://localhost:11434/api/generate"
    
    # Configurazione ottimizzata per qualit√† e cooperazione
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.4,        # Leggermente pi√π alto per creativit√†
            "top_p": 0.9,             # Pi√π ampio per variet√†
            "top_k": 50,              # Pi√π scelte disponibili
            "num_predict": 2048,      # Output pi√π lungo per strategie complete
            "repeat_penalty": 1.1,    # Evita ripetizioni
            "num_ctx": 4096,          # Contesto pi√π ampio
            "num_thread": 8,          # Usa pi√π thread se disponibili
            "num_gpu": 1,             # Usa GPU se disponibile
            "num_batch": 512,         # Batch size ottimizzato
            "rope_freq_base": 10000,  # Parametri ROPE ottimizzati
            "rope_freq_scale": 0.5
        }
    }
    
    try:
        print(f"ü§ù Invio richiesta cooperativa a {model} (senza timeout)...")
        response = requests.post(url, json=payload, timeout=36000)  # 10 ore di timeout
        response.raise_for_status()
        result = response.json()
        print(f"‚úÖ Risposta cooperativa ricevuta da {model}")
        return result["response"]
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Errore nella richiesta cooperativa a {model}: {e}")
        raise

def query_ollama_fast(prompt: str, model: str = "phi3", timeout: int = 600) -> str:
    """
    Versione ultra-veloce per prompt semplici e decisioni rapide.
    Usa phi3 che √® pi√π veloce per operazioni semplici.
    """
    url = "http://localhost:11434/api/generate"
    
    # Configurazione ultra-veloce
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.1,        # Molto deterministico
            "top_p": 0.5,             # Molto focalizzato
            "num_predict": 512,       # Output molto breve
            "num_ctx": 1024,          # Contesto minimo
            "num_thread": 8,
            "num_batch": 256
        }
    }
    
    try:
        print(f"‚ö° Invio richiesta veloce a {model}...")
        response = requests.post(url, json=payload, timeout=timeout)
        response.raise_for_status()
        result = response.json()
        print(f"‚úÖ Risposta veloce ricevuta da {model}")
        return result["response"]
    except Exception as e:
        print(f"‚ùå Errore nella richiesta veloce a {model}: {e}")
        raise

def query_ollama_cooperative(prompt: str, model: str = "cogito:8b", session_id: str = None) -> str:
    """
    Versione specializzata per cooperazione tra LLM.
    Ottimizzata per generazione di strategie complesse e interazioni cooperative.
    
    Args:
        prompt: Il prompt da inviare al modello
        model: Il nome del modello da utilizzare (default: cogito:8b per cooperazione)
        session_id: ID della sessione cooperativa per logging
        
    Returns:
        Risposta del modello ottimizzata per cooperazione
    """
    url = "http://localhost:11434/api/generate"
    
    # Configurazione ottimizzata per cooperazione
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.5,        # Bilanciato per creativit√† e coerenza
            "top_p": 0.85,            # Ampio ma controllato
            "top_k": 45,              # Buona variet√†
            "num_predict": 3072,      # Output molto lungo per strategie complete
            "repeat_penalty": 1.15,   # Evita ripetizioni
            "num_ctx": 8192,          # Contesto molto ampio per cooperazione
            "num_thread": 8,          # Usa pi√π thread se disponibili
            "num_gpu": 1,             # Usa GPU se disponibile
            "num_batch": 1024,        # Batch size maggiore per cooperazione
            "rope_freq_base": 10000,  # Parametri ROPE ottimizzati
            "rope_freq_scale": 0.5
        }
    }
    
    try:
        session_info = f" (sessione: {session_id})" if session_id else ""
        print(f"ü§ù Invio richiesta cooperativa a {model}{session_info}...")
        response = requests.post(url, json=payload, timeout=36000)  # 10 ore di timeout per cooperazione
        response.raise_for_status()
        result = response.json()
        print(f"‚úÖ Risposta cooperativa ricevuta da {model}{session_info}")
        return result["response"]
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Errore nella richiesta cooperativa a {model}: {e}")
        raise

def test_model_availability(model: str = "mistral") -> bool:
    """
    Testa se un modello √® disponibile e risponde.
    
    Args:
        model: Il nome del modello da testare
        
    Returns:
        True se il modello √® disponibile, False altrimenti
    """
    try:
        test_prompt = "Rispondi solo con 'OK'"
        result = query_ollama_fast(test_prompt, model, timeout=120)
        return "OK" in result.upper()
    except Exception as e:
        print(f"‚ùå Modello {model} non disponibile: {e}")
        return False

def get_available_models() -> List[str]:
    """
    Restituisce la lista dei modelli disponibili.
    
    Returns:
        Lista dei nomi dei modelli disponibili
    """
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        response.raise_for_status()
        data = response.json()
        return [model["name"] for model in data.get("models", [])]
    except Exception as e:
        print(f"‚ùå Errore nel recupero modelli: {e}")
        return []

def get_model_speed_ranking() -> List[str]:
    """
    Restituisce i modelli ordinati per velocit√† (dal pi√π veloce al pi√π lento).
    
    Returns:
        Lista ordinata dei modelli per velocit√†
    """
    # phi3 √® il pi√π veloce, mistral √® il pi√π lento ma pi√π accurato
    return ["phi3", "llama2", "mistral"]

def get_model_timeout_config() -> Dict[str, Dict[str, int]]:
    """
    Restituisce la configurazione dei timeout ottimali per ogni modello.
    Basato su test reali di velocit√† dei modelli.
    """
    return {
        "phi3": {
            "fast": 300,      # 5 minuti per prompt semplici
            "normal": 600,    # 10 minuti per prompt normali
            "complex": 900    # 15 minuti per prompt complessi
        },
        "llama2": {
            "fast": 600,      # 10 minuti per prompt semplici
            "normal": 1200,   # 20 minuti per prompt normali
            "complex": 1800   # 30 minuti per prompt complessi
        },
        "mistral": {
            "fast": 900,      # 15 minuti per prompt semplici
            "normal": 1800,   # 30 minuti per prompt normali
            "complex": 2700   # 45 minuti per prompt complessi
        }
    }

def get_optimal_timeout(model: str, prompt_complexity: str = "normal") -> int:
    """
    Calcola il timeout ottimale basato sul modello e sulla complessit√† del prompt.
    
    Args:
        model: Nome del modello LLM
        prompt_complexity: Complessit√† del prompt ("fast", "normal", "complex")
        
    Returns:
        Timeout in secondi
    """
    config = get_model_timeout_config()
    
    # Default per modelli sconosciuti
    if model not in config:
        return 1800  # 30 minuti di default
    
    return config[model].get(prompt_complexity, config[model]["normal"])

def estimate_prompt_complexity(prompt: str) -> str:
    """
    Stima la complessit√† di un prompt basandosi su vari fattori.
    
    Args:
        prompt: Il prompt da analizzare
        
    Returns:
        Complessit√† stimata ("fast", "normal", "complex")
    """
    # Fattori per determinare la complessit√†
    length = len(prompt)
    has_code = "def " in prompt or "class " in prompt or "import " in prompt
    has_instructions = prompt.count("Includi") + prompt.count("Genera") + prompt.count("Crea")
    
    if length < 200 and not has_code and has_instructions < 2:
        return "fast"
    elif length < 800 and has_instructions < 5:
        return "normal"
    else:
        return "complex" 